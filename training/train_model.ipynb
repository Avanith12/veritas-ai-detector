{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AI vs Real Image Detector Training Notebook\n",
                "\n",
                "This notebook will guide you through training a Convolutional Neural Network (CNN) to detect AI-generated images.\n",
                "\n",
                "## 1. Setup & Dataset\n",
                "We will automatically download the **CIFAKE** dataset using `kagglehub`. This dataset contains labeled Real and AI images perfect for our task.\n",
                "\n",
                "### Instructions:\n",
                "Just run the cells below! The dataset will be downloaded for you."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import os\n",
                "import PIL\n",
                "import tensorflow as tf\n",
                "import kagglehub\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "from tensorflow.keras.models import Sequential\n",
                "import pathlib\n",
                "\n",
                "# Automatic Dataset Download\n",
                "print(\"Downloading CIFAKE dataset...\")\n",
                "path = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n",
                "print(\"Path to dataset files:\", path)\n",
                "\n",
                "# Check for GPU availability\n",
                "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define parameters\n",
                "batch_size = 32\n",
                "img_height = 32\n",
                "img_width = 32\n",
                "\n",
                "# Path to the dataset (Using the downloaded path)\n",
                "data_dir = pathlib.Path(path) / 'train'\n",
                "\n",
                "if not data_dir.exists():\n",
                "    print(\"WARNING: Dataset directory not found at {}. Please compare with instructions above.\".format(data_dir.resolve()))\n",
                "else:\n",
                "    print(\"Dataset directory found at:\", data_dir)\n",
                "\n",
                "# Load Training Data\n",
                "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
                "  data_dir,\n",
                "  validation_split=0.2,\n",
                "  subset=\"training\",\n",
                "  seed=123,\n",
                "  image_size=(img_height, img_width),\n",
                "  batch_size=batch_size)\n",
                "\n",
                "# Load Validation Data\n",
                "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
                "  data_dir,\n",
                "  validation_split=0.2,\n",
                "  subset=\"validation\",\n",
                "  seed=123,\n",
                "  image_size=(img_height, img_width),\n",
                "  batch_size=batch_size)\n",
                "\n",
                "class_names = train_ds.class_names\n",
                "print(\"Class names:\", class_names)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the data\n",
                "try:\n",
                "    plt.figure(figsize=(10, 10))\n",
                "    for images, labels in train_ds.take(1):\n",
                "      for i in range(9):\n",
                "        ax = plt.subplot(3, 3, i + 1)\n",
                "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
                "        plt.title(class_names[labels[i]])\n",
                "        plt.axis(\"off\")\n",
                "except Exception as e:\n",
                "    print(\"Could not visualize data (dataset might be missing):\", e)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Build the Model\n",
                "We will use a Sequential model with three Convolutional blocks followed by a Dense layer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_classes = len(class_names)\n",
                "\n",
                "model = Sequential([\n",
                "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
                "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
                "  layers.MaxPooling2D(),\n",
                "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
                "  layers.MaxPooling2D(),\n",
                "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
                "  layers.MaxPooling2D(),\n",
                "  layers.Flatten(),\n",
                "  layers.Dense(128, activation='relu'),\n",
                "  layers.Dense(num_classes)\n",
                "])\n",
                "\n",
                "model.compile(optimizer='adam',\n",
                "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the model\n",
                "epochs = 10\n",
                "history = model.fit(\n",
                "  train_ds,\n",
                "  validation_data=val_ds,\n",
                "  epochs=epochs\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Visualize Training Results\n",
                "try:\n",
                "    acc = history.history['accuracy']\n",
                "    val_acc = history.history['val_accuracy']\n",
                "\n",
                "    loss = history.history['loss']\n",
                "    val_loss = history.history['val_loss']\n",
                "\n",
                "    epochs_range = range(epochs)\n",
                "\n",
                "    plt.figure(figsize=(12, 12))\n",
                "    plt.subplot(2, 1, 1)\n",
                "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
                "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
                "    plt.legend(loc='lower right')\n",
                "    plt.title('Training and Validation Accuracy')\n",
                "\n",
                "    plt.subplot(2, 1, 2)\n",
                "    plt.plot(epochs_range, loss, label='Training Loss')\n",
                "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
                "    plt.legend(loc='upper right')\n",
                "    plt.title('Training and Validation Loss')\n",
                "    plt.show()\n",
                "except NameError:\n",
                "    print(\"Training history not found. Train the model first!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Save the Model\n",
                "Once training is complete, we save the model so our Web App can use it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save('ai_detector_model.h5')\n",
                "print(\"Model saved as ai_detector_model.h5\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}